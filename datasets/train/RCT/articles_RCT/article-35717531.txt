Randomized trial of the effect of video training on residents’ surgical skills in facial skin reconstructive surgery: A SQUIRE study
1. Introduction
To master a given surgical procedure, residents receive 2 types of training. Theoretic training covers the steps and pitfalls of the procedure; practical training uses anatomic specimens, simulators and hospital patients. Theoretic training is constantly progressing and needs to be improved so that residents can confidently move on to practical training and actual surgery. The classic methods, with atlases, manuals and articles, do not seem to be sufficient. Pugh et al. [1] reported that 71 American students in their 1st and 5th years of residency claimed to be poorly prepared after studying from atlases and surgical articles. Programs with shorter workload incurred a significant decrease in the number of procedures students performed according to American studies conducted after 2003, when time spent working in hospital was reduced by law by 80 hours [2], [3]. Surgical videos were therefore considered as an important training tool, with strong demand from residents themselves [4], [5], and were found to be effective in improving theoretic training for common procedures such as laparoscopic colectomy [6], basic surgical techniques [7] and neck dissection [8].

In 2018, a study assessed the impact of interactive surgical simulations via smartphone for facial skin reconstruction surgery. These were not exactly training videos, however, and assessment was based on a multiple-choice questionnaire [9]. To the best of our knowledge, no studies assessed the impact of surgical videos in facial skin reconstruction training, although this seems an area in which videos could be useful, as many practitioners perform such reconstruction after skin tumor resection: dermatologists, otorhinolaryngologists and head and neck surgeons, maxillofacial surgeons and plastic surgeons.

The aim of the present study was to assess the impact of facial skin reconstruction training videos for residents.

2. Material and methods
This randomized study was performed with ENT and head and neck and maxillofacial surgery residents in Paris, France, between May and July 2021. A questionnaire was sent to 50 head and neck and maxillofacial surgery residents asking about how they prepared an operation. They were told that the first 18 respondents would be selected for a 1-day facial skin reconstruction training session in the Fer à Moulin surgery school in Paris [10]. The questionnaire collected basic data about the students: age, residency semester, experience with local flaps for facial reconstruction, how they prepared surgery and the time spent on this, and the availability of facial skin reconstruction videos and techniques.

A website was set up with forty or so videos for resident and surgeon training in various facial reconstruction techniques after skin cancer resection (www.chirurgiedelafacepontoise.com). Four videos of flaps were selected: Camille Bernard, forehead, Mustarde and Antia Buch. The videos adhered to IVORY (Instructional Videos in ORL by YO-IFOS) guidelines for educational videos in head and neck surgery [11].

All 18 residents received a 60-minute presentation of several facial skin reconstruction techniques based on atlases, manuals and articles, by a surgeon experience in facial skin reconstruction. The 4 flaps that would be used on the day of dissection were included in the presentation. At this point in time, the students did not know which flap they would view and/or perform on the day of dissection. They came to the Fer à Moulin surgery school in Paris for 2 separate practical days. Three days before, they were allocated to 1 of 2 groups of 9: Group A performed Mustarde and forehead flaps, and Group B Camille Bernard and Antia-Buch flaps. Residents in Group A did not know the videos to be viewed by Group B, and vice-versa.

In the dissection session, each student performed 4 locoregional head flaps on specimens: forehead, Mustarde, Camille Bernard and Antia-Buch. For each flap they were allocated to a “video” or a “no video” group, depending on whether they had seen the appropriate video ahead of dissection (Fig. 1). An examiner (surgeon experience in facial flaps,) present during dissection, drew up a grading form in advance and filled it out during the dissections, blind to the student's group (“video” or “no video”), grading 0-5 for flap drawing, 0-5 for the steps of flap creation, and 0-5 for the overall aspect of the reconstruction. Zeros corresponded to inability to perform the drawing, step or reconstruction, and 5s corresponded to perfect drawing, steps perfectly executed without the examiner's assistance, and perfect reconstruction aspect. The 3 scores were summed in a global score out of 15 per resident per flap (Fig. 2). The main study endpoint was the difference in global score between the video and no-video groups.
Scores were compared between video and no-video groups, with each resident figuring twice in each group, thus serving as his or her own control. Residents could ask for help during dissection, and thus number of such requests was noted.

The Mann-Whitney U test was used to compare scores between groups, for each flap taken separately and then for all flaps taken together. The significance threshold was set at P < 0.005, with p-values between 0.05 and 0.005 considered suggestive, in line with the reforms supported by European Annals of Otorhinolaryngology Head & Neck Diseases. [12] [13], [14]. A 20% intergroup difference (3 out of 15 points) was considered clinically relevant. With 56 procedures (28 per group), study power was calculated to be >98% with first-order risk of 0.05.

The article was written up in line with the SQUIRE-EDU (Standards for QUality Improvement Reporting Excellence in EDUcation) guidelines on article writing to improve scientific education [15]. The anonymity of the patients featuring in the videos was respected, and consent for publication was secured by the examiner in dedicated consultation.

3. Results
3.1. Population
Questionnaire responses were collected for 23 residents. All reported using books or surgery lessons to prepare for the procedures. Sixteen reported using surgery videos found on the Internet. All thought the videos were “very useful” for preparation, but finding facial skin reconstruction videos was reported to be “difficult” by 6, “quite difficult” by 14 and “quite easy” by 2; none claimed that it was “easy”. Five reported no prior experience with local and regional facial flaps, 5 reported very little experience, 11 little experience and 2 quite good experience; none reported “good” prior experience.

The study included 18 residents, who all received a 60-minute presentation on facial skin reconstruction, including the 4 flaps they went on to use. Half (n = 9) were randomized to group A, visualizing the Mustarde and forehead flaps, and the other half to group B, visualizing the Antia Buch and Camille Bernard flaps. Each performed all 4 flaps, for a total 72 flaps (Fig. 1).

Each resident was simultaneously in “video” and “no-video” groups, acting as their own control. Baseline characteristics were thus strictly identical between groups. Groups A and B were compared in terms of year of residency semester and experience with local facial reconstruction flaps; no significant intergroup differences emerged: P = 0.14 and P = 0.46, respectively.

3.2. Score out of 15 according to group
Scores out of 15 were significantly higher in the “video” group: 6 [IQR, 4: 9] versus 10 [9: 12]; P < 0.001. This corresponded to the main study endpoint. The range was 4–15 in the “video” group and 0–14 in the “no-video” group (Fig. 3).
3.3. Scores out of 5 according to group
Scores out of 5 were likewise significantly higher in the “video” group, for flap drawing (1 [0: 3] vs. 3 [2: 4]; P < 0.001), and surgical steps (2 [1: 3] vs. 4 [3: 4]; P < 0.001), while the examiner's scores out of 5 for global reconstruction aspect were suggestively but not significantly higher in the “video” group: 3 [2: 3] vs. 4 [3: 4]; P < 0.01.

3.4. Assistance by the examiner according to group
Residents in the “no-video” group requested assistance significantly more often (3 [2: 4] vs. 1 [1: 2]; P < 0.001). This corresponded to the secondary study endpoint.

3.5. Subgroup analysis according to semester of residency
Scores out of 15 were significantly higher in the “video” group for 2nd-year residents (7 [5: 7] vs. 12 [11: 13]; P < 0.001), and suggestively, but not significantly, higher for 1st-year residents. (4 [2: 6] vs. 9 [7: 10]; P < 0.01); for 3rd and 4th-year residents, there were no inter group differences (10 [9: 12] vs. 9 [6: 10]; P = 0.21).

3.6. Subgroup analysis according to flap
Scores out of 15 were suggestively, but not significantly, higher in the “video” group for the Camille Bernard and Mustarde flaps (5 [4: 5] vs. 11.0 [9: 11]; P < 0.01; and 8 [5: 9] vs. 11 [10: 13]; P = 0.014, respectively). For the forehead and Antia-Buch, there were no intergroup differences (6 [2: 8] vs. 9 [7: 10]; P =0.083; and 6 [6: 9] vs. 12 [9: 13]; P = 0.062, respectively).

4. Discussion
The present randomized controlled study assessed the impact of facial skin reconstruction surgery training videos for residents. Surgical videos are increasingly seen as a useful means of enhancing residents’ surgical skills, and have met with great interest in recent years. A PubMed search using the keywords “video”, “teaching” and “surgery” retrieves more than 5300 results to date, especially for the last 10 years. Some studies advocated surgery-training videos, but very few assessed their actual impact on residents’ skills [4], [5], [6], [7]. Studies of training using video have been reported in various fields, but not in facial skin reconstruction, with only 6 results on PubMed for “video”, “teaching” and “skin plastic surgery”, and these did not assess the impact of video. The present study therefore took this as its focus.

The strong points of the study firstly concern its randomized controlled design, with each resident performing surgery both after and without viewing the explanatory video, thus serving as his or her own control. Secondly, the examiner was blind to whether the student had viewed the video, and was thus not biased in attributing grades. And finally, performing local and regional skin flaps on cadaver specimens in the Paris surgery school was technically close to real-life conditions.

The study found a positive impact of surgical video by comparing dissection scores according to whether the resident had viewed the corresponding video. The impact was also seen in the number of times residents asked for help during dissection. The study lacked power to reveal significant differences on subgroup analysis according to residency year and specific flaps. The results agreed with previous reports in other specialties. Hayden et al. [4] and Poon et al. [5] reported that surgical videos were attractive for residents, but did not assess their actual usefulness. The present questionnaire results agreed with previous reports. Crawshaw et al. [6] reported better performance after viewing a colectomy video, in 54 procedures. Farquharson et al. [7] showed that video could enhance basic surgical skills, in a study of 48 students; however, this concerned basic skills such as familiarity with instruments and suture. Mendez et al. [8] found a decreased error rate between 2 neck dissections, before and after viewing a surgical video, in 6 residents.

These findings should encourage further studies to confirm the contribution of training videos in facial skin reconstruction. They could provide an inexpensive and reproducible learning aid.

Extrapolation of the present results encounters limitations. Long-term performance and memorization were not assessed at a distance in time from viewing the videos. Reassessment at a later date could have assessed memorization. One advantage of surgical videos, on the other hand, is that the student is able to view them as often as they wish. Another limitation was that the study was performed not in a clinical setting but on specimens in the surgery school, so that the results are not directly transposable to real-life situations. Moreover, the videos had not been formally validated by qualitative assessment. However, they did meet the IVORY criteria [10] for videos of surgical techniques, which represent an expert consensus for laying down guidelines for educational videos in head and neck surgery.

There was also an interpretation bias. The cohort was small, with 18 residents, although each performed 4 procedures. Subgroup analyses according to year of residency and to the particular flap lacked statistical power and were unable to demonstrate significant intergroup differences. There was also a confusion bias in that residents could work on the procedures by other means than the videos (articles, books, etc.) once they knew 2 of the flaps they were going to perform, diminishing the contribution of the video to their success. Also, when residents performed, for example, a forehead flap without having viewed the corresponding video, they had received teaching on facial flaps but not specifically on the flap in question. However, the teaching presentations did include description of several flap techniques, including the 4 performed during the dissection session. Moreover, it was difficult to assess individual levels, apart from in terms of years of study and age; there may thus have been a selection bias related to possible differences in surgical skills between groups A and B. However, analysis was based not on groups A and B (according to visualized flap) but on “video” and “no-video” groups, and each resident figured in both groups, abolishing any such selection bias. And lastly, the first-order risk used to calculate sample size was 0.05, whereas, in line with the methodological improvements advocated by the European Annals of Otorhinolaryngology Head & Neck Diseases, the significance threshold was set at P < 0.005.

5. Conclusion
Videos improved surgery residents’ performances in dissection. They are not meant to replace theoretic and practical training, but are an inexpensive and reproducible learning aid that does not involve patient recruitment. However, the present results cannot be extrapolated to real-life clinical settings; they require validation by a larger-scale study assessing performance in actual surgery. Further evidence could help spread the use of video for surgery residents’ training.